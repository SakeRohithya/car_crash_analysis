{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def read_yaml(file_path):\n",
    "    '''\n",
    "        Reads data from config yaml file\n",
    "\n",
    "        Parameters : \n",
    "            file_path : File path to yaml config file\n",
    "\n",
    "        Returns : \n",
    "            Dict with config details\n",
    "\n",
    "    '''\n",
    "    try : \n",
    "        with open(file_path, 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "    except Exception as e : \n",
    "        print(f'Error occurred while loading data from yaml file with following exception {e}')\n",
    "\n",
    "\n",
    "def read_csv_data(spark, file_path):\n",
    "    '''\n",
    "        Reads data in csv format\n",
    "\n",
    "        Parameters : \n",
    "            spark : Spark instance \n",
    "            file_path : File path containing csv data\n",
    "        \n",
    "        Returns : \n",
    "            Dataframe : Spark Dataframe containing input csv file data\n",
    "    '''\n",
    "    try : \n",
    "        return spark.read.option('header', 'True').option('inferSchema', 'true').csv(file_path)\n",
    "\n",
    "    except Exception as e : \n",
    "        print(f'Error occurred while reading data from file with following exception {e}')\n",
    "\n",
    "\n",
    "\n",
    "def write_csv_data(df, file_path):\n",
    "    '''\n",
    "        Write output dataframe to csv file\n",
    "\n",
    "        Parameters : \n",
    "            df : Dataframe containing the results\n",
    "            file_path : Path to file in which data needs to be stored\n",
    "\n",
    "        Returns : \n",
    "            None\n",
    "    '''\n",
    "    try : \n",
    "        df.write.coalesce(1).format('csv').mode('overwrite').option('header', 'true').save(file_path)\n",
    "    \n",
    "    except Exception as e : \n",
    "        print(f'Error occurred while writing df data to csv file with following exception {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from utilities import read_yaml , read_csv_data  , write_csv_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit , col , dense_rank , sum , count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/07 23:12:25 WARN Utils: Your hostname, Sigmoids-MacBook-Air-4.local resolves to a loopback address: 127.0.0.1; using 192.168.29.131 instead (on interface en0)\n",
      "24/01/07 23:12:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/07 23:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('CarCrashAnalysis') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = '../config.yaml'\n",
    "    # spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source_paths = read_yaml(config_file_path)['input_source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'csv',\n",
       " 'Charges': '../Data/Charges_use.csv',\n",
       " 'Damages': '../Data/Damages_use.csv',\n",
       " 'Endorse': '../Data/Endorse_use.csv',\n",
       " 'Primary_Person': '../Data/Primary_Person_use.csv',\n",
       " 'Restrict': '../Data/Restrict_use.csv',\n",
       " 'Unit': '../Data/Units_use.csv'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_source_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_paths = read_yaml(config_file_path)['output_destination']['analysis1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "charges_df = read_csv_data(spark, input_source_paths['Charges'])\n",
    "damages_df = read_csv_data(spark, input_source_paths['Damages'])\n",
    "endorse_df = read_csv_data(spark, input_source_paths['Endorse'])\n",
    "primary_person_df = read_csv_data(spark, input_source_paths['Primary_Person'])\n",
    "units_df = read_csv_data(spark, input_source_paths['Unit'])\n",
    "restrict_df = read_csv_data(spark, input_source_paths['Restrict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges_speed_filtered_df = charges_df.filter(col('CHARGE').like('%SPEED%')).select('CRASH_ID','UNIT_NBR','CHARGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges_speed_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_person_licensed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_person_licensed_df = primary_person_df.filter(~col('DRVR_LIC_CLS_ID').like('UNLICENSED')).select('CRASH_ID','UNIT_NBR', 'DRVR_LIC_CLS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_top10_color_df = units_df.filter(col('VEH_COLOR_ID')!='NA').groupBy('VEH_COLOR_ID').agg(count('*').alias('VEH_COLOR_COUNT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_color = Window().orderBy(col('VEH_COLOR_COUNT').desc())\n",
    "units_top10_color_df = units_top10_color_df.withColumn('TOP10_VEH_COLOR', dense_rank().over(spec_color)).filter(col('TOP10_VEH_COLOR')<=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_top25_state_df = units_df.filter(col('VEH_LIC_STATE_ID')!='NA').groupBy('VEH_LIC_STATE_ID').agg(count('*').alias('VEH_STATE_COUNT'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_state = Window().orderBy(col('VEH_STATE_COUNT').desc())\n",
    "units_top25_state_df = units_top25_state_df.withColumn('TOP25_VEH_STATE', dense_rank().over(spec_state)).filter(col('TOP25_VEH_STATE')<=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_filtered_color_state_df = units_df.filter(col('VEH_COLOR_ID').isin([i[0] for i in units_top10_color_df.collect()])) \\\n",
    "    .filter(col('VEH_LIC_STATE_ID').isin([i[0] for i in units_top25_state_df.collect()])).select('CRASH_ID' , 'UNIT_NBR', 'VEH_MAKE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(charges_speed_filtered_df.count(),primary_person_licensed_df.count(),  units_filtered_color_state_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_units_charges_licensed_drivers_df = units_filtered_color_state_df.join(charges_speed_filtered_df , on=['CRASH_ID' , 'UNIT_NBR'], how='inner') \\\n",
    "    .join(primary_person_licensed_df , on=['CRASH_ID' , 'UNIT_NBR'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_units_charges_licensed_drivers_df = final_units_charges_licensed_drivers_df.groupBy('VEH_MAKE_ID').agg(count('*').alias('VEH_MAKE_ID_COUNT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_make_id = Window().orderBy(col('VEH_MAKE_ID_COUNT').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_units_charges_licensed_drivers_df = final_units_charges_licensed_drivers_df.withColumn('RANK', dense_rank().over(spec_make_id)).filter(col('RANK')<=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_units_charges_licensed_drivers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[0] for i in final_units_charges_licensed_drivers_df.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_data(final_units_charges_licensed_drivers_df, output_file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i[0] , i[1]) for i in top_ethinc_vehicle_body_wise.collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarCrashAnalysis:\n",
    "    def __init__(self, config_file_path):\n",
    "        input_source_paths = read_yaml(config_file_path)['input_source']\n",
    "        self.charges_df = read_csv_data(spark, input_source_paths['Charges'])\n",
    "        self.damages_df = read_csv_data(spark, input_source_paths['Damages'])\n",
    "        self.endorse_df = read_csv_data(spark, input_source_paths['Endorse'])\n",
    "        self.primary_person_df = read_csv_data(spark, input_source_paths['Primary_Person'])\n",
    "        self.units_df = read_csv_data(spark, input_source_paths['Unit'])\n",
    "        self.restrict_df = read_csv_data(spark, input_source_paths['Restrict'])\n",
    "\n",
    "    def count_male_accidents(self, output_destination_path):\n",
    "        \"\"\"\n",
    "        Finds the crashes (accidents) in which number of persons killed are male\n",
    "        :param output_path: output file path\n",
    "        :param output_format: Write file format\n",
    "        :return: dataframe count\n",
    "        \"\"\"\n",
    "        df = self.primary_person_df.filter(self.primary_person_df.PRSN_GNDR_ID == \"MALE\")\n",
    "        write_csv_data(df, output_destination_path)\n",
    "        return df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Initialize sparks session\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('CarCrashAnalysis') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    config_file_path = 'config.yaml'\n",
    "    # spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    usvaa = CarCrashAnalysis(config_file_path)\n",
    "    output_file_paths = read_yaml(config_file_path)['output_destination']['analysis1']\n",
    "    # file_format = read_yaml(config_file_path).get(\"FILE_FORMAT\")\n",
    "\n",
    "    # 1. Find the number of crashes (accidents) in which number of persons killed are male?\n",
    "    print(\"1. Result:\", usvaa.count_male_accidents(output_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source_paths = read_yaml(config_file_path)['input_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_person_df = read_csv_data(spark, input_source_paths['Primary_Person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_person_df = primary_person_df.repartition(1)\n",
    "primary_person_df.write.format('csv').mode('overwrite').option('header', 'true').save(output_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
